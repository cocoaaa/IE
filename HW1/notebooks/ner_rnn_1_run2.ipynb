{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1. NER with LSTM + FC layer\n",
    "For Part 2, I tried two different models. The first model is a LSTM unit + fully-connected layer and the second model is a Bidirectional LSTM unit with CRF. The second model learns the word embeddings through bidirectional LSTM and use the output word vectors as input sequences to the CRF based model to predict the probability over the tag sequences and the weights on features.  This notebook focuses on the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..', '', '/home/hayley/miniconda3/envs/fastai/lib/python36.zip', '/home/hayley/miniconda3/envs/fastai/lib/python3.6', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/lib-dynload', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/defusedxml-0.5.0-py3.6.egg', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/IPython/extensions', '/home/hayley/.ipython', '..']\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import random\n",
    "print(sys.path)\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "# sklearn imports\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict\n",
    "\n",
    "# sklearn_crfsuite imports\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# train logging\n",
    "import logging\n",
    "from tqdm import trange\n",
    "# import .utils as my_utils\n",
    "from nlp_utils.model_evaluate import evaluate\n",
    "from nlp_utils import model_utils\n",
    "# set a random seed\n",
    "torch.manual_seed(10);\n",
    "\n",
    "# model saving and inspection\n",
    "import joblib\n",
    "import eli5\n",
    "\n",
    "import pdb \n",
    "\n",
    "# auto-reloads\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.20.0\n",
      "pytorch version: 0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "print(f\"pytorch version: {torch.__version__}\")\n",
    "# make sure we are using pytorch > 0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/hayley/miniconda3/envs/fastai/lib/python36.zip', '/home/hayley/miniconda3/envs/fastai/lib/python3.6', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/lib-dynload', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/defusedxml-0.5.0-py3.6.egg', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/IPython/extensions', '/home/hayley/.ipython', '..']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra word for PAD (padding) and UNK (unrecognized word)\n",
    "PAD_WORD = '<PAD>'\n",
    "PAD_TAG = 'O'\n",
    "UNK_WORD = '<UNK>'\n",
    "\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the CRF models, we don't need to hand-engineer the features for our word representation. We use the index mapper (from word to index and tag to index) as we defined in `create_vocab.ipynb` and in the Step section above. To summarize, we use the most common $N$ words out of the total words that occured in all the datasets provided (`eng:train`, `eng:testa`, `eng:testb`). In our experiments $N$ is set $15,000$ which is half of the number of words that occured at least once in any of the given datasets. I added several extra words: PAD_WORD and UNK_WORD. PAD_WORDs are appended to sentences in a mini-batch to make every sentence of equal length (as to the length of the longest sentence in the mini-batch). UNK_WORD is used to map words that are not in our vocab (because we excluded 15,000 uncommon words). In addition, I assigned a PAD_TAG to indicate the padding words, and START_TAG and STOP_TAG. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed sentences and labels as well as the word2idx and tag2idx dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = joblib.load('../data/train_sentences.sav')\n",
    "train_labels = joblib.load('../data/train_labels.sav')\n",
    "\n",
    "dev_sentences = joblib.load('../data/testa_sentences.sav')\n",
    "dev_labels = joblib.load('../data/testa_labels.sav')\n",
    "\n",
    "test_sentences = joblib.load('../data/testb_sentences.sav')\n",
    "\n",
    "# indice mappers\n",
    "word2idx = joblib.load('../data/word2idx.sav')\n",
    "tag2idx = joblib.load('../data/tag2idx.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentences:  10490 10490\n",
      "dev sentences:  3464 3464\n",
      "test sentences:  3683\n",
      "vocab size:  15002\n",
      "number of tags:  11\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics on the datasets and lookup tables\n",
    "print('train sentences: ', len(train_sentences), len(train_labels))\n",
    "print('dev sentences: ', len(dev_sentences), len(dev_labels))\n",
    "print('test sentences: ', len(test_sentences))\n",
    "print('vocab size: ', len(word2idx))\n",
    "print('number of tags: ', len(tag2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data loaders for mini-batch of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we sample a batch of sentences, the sentences usually have a different length. In a batch of sentences, (`batch_sentences`) with correspoonding batch of tags `batch_tags`, we add PAD_WORD for sentences that have fewer words than SQE_LENGTH (set to maximum length of a sentence in the current `batch_sentences`). Below shows this processing in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3929c630a0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is just to show the processing and is not meant to actually run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Maximum sentence lengths in current batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatch_max_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Intial matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# This is just to show the processing and is not meant to actually run.\n",
    "# Maximum sentence lengths in current batch \n",
    "batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "# Intial matrix\n",
    "batch_data = word2idx[PAD_WORD]*np.ones((len(batch_sentences), batch_max_len))\n",
    "batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "# Fill in the matrix with current batch sentences and labels\n",
    "for i in range(len(batch_sentences)):\n",
    "    curr_len = len(batch_sentences[i])\n",
    "    batch_data[i][:curr_len] = batch_sentences[i]\n",
    "    batch_labels[i][:curr_len] = batch_tags[i]\n",
    "\n",
    "# Convert to torch.LongTensors (since each entry is an index)\n",
    "batch_data = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the processing above, our `batch_data` now consists of the same length of sequences,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataLoader(object):\n",
    "    \"\"\"\n",
    "    Loads a mini-batch of data at each iterations. \n",
    "    Stores the following properties:\n",
    "    - dataset_params\n",
    "    - word2idx: word to index mapping\n",
    "    - tag2idx: tag to index mapping\n",
    "    \n",
    "    Args:\n",
    "    - data_dir (str): path to the directory containing the dataset\n",
    "    - parsm (dict): hyperparameters for data loading\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, word2idx_file, tag2idx_file):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        word2idx_path = os.path.join(data_dir, word2idx_file)\n",
    "        tag2idx_path = os.path.join(data_dir, tag2idx_file)\n",
    "        self.word2idx = joblib.load(word2idx_path)\n",
    "        self.tag2idx = joblib.load(tag2idx_path)\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.n_tags = len(self.tag2idx)\n",
    "        \n",
    "    def load_sentences_labels(self, sentences_path, labels_path, d):\n",
    "        \"\"\"\n",
    "        Load sentences and labels for this dataset to the input dictionary d\n",
    "        \"\"\"\n",
    "        d['data'] = joblib.load(sentences_path)\n",
    "        d['labels'] = joblib.load(labels_path)\n",
    "        d['size'] = len(d['data'])\n",
    "        \n",
    "        \n",
    "    def load_data(self,types):\n",
    "        \"\"\"\n",
    "        Load dataset(s) from data_dir.\n",
    "        Args:\n",
    "        - data_dir (str): path to the directory that contains dataset files\n",
    "        - types (list): a list of string(s) which is one of 'train', 'dev', 'test'\n",
    "        \n",
    "        Returns:\n",
    "        - data (dict): contains the sentences and labels for each type in types\n",
    "        \"\"\"            \n",
    "        data = {}\n",
    "        for split in ['train', 'dev', 'testa']:\n",
    "            if split in types:\n",
    "                sentences_path = os.path.join(self.data_dir, split+\"_sentences.sav\")\n",
    "                labels_path = os.path.join(self.data_dir, split+\"_labels.sav\")\n",
    "                print(sentences_path, \"\\n\", labels_path)\n",
    "                                     \n",
    "                data[split] = {}\n",
    "                self.load_sentences_labels(sentences_path, labels_path, data[split])\n",
    "        return data\n",
    "    \n",
    "    def data_iterator(self, data, params, shuffle=False):\n",
    "        \"\"\"\n",
    "        Returns a generator that yields a mini-batch of data (sentences and labels).\n",
    "        It iteratates once over the data\n",
    "        \n",
    "        Args:\n",
    "        - data (dict): a dictionary with keys of 'data', 'labels', 'size'\n",
    "        - params (dict): hyperparams of the training \n",
    "        - shuffle (bool): to shuffle the mini-batch or not\n",
    "        \n",
    "        Yields:\n",
    "        - batch_data (torch.LongTensor): word indices of size batch_size * seq_len \n",
    "        - batch_labels (torch.LongTensor): tag indices of size batch_size * seq_len\n",
    "        \"\"\"\n",
    "        order = list(range(data['size']))\n",
    "        if shuffle:\n",
    "            random.seed(0)\n",
    "            random.shuffle(order)\n",
    "        \n",
    "        # One iteration over data\n",
    "        for i in range( (data['size']+1)//params.batch_size ):\n",
    "            # Get a batch of sentences and tags \n",
    "            batch_sentences = [data['data'][i] for i in order[i*params.batch_size: (i+1)*params.batch_size]]\n",
    "            batch_tags = [data['labels'][i] for i in order[i*params.batch_size: (i+1)*params.batch_size]]\n",
    "            \n",
    "            # Perform the two modification mentioned above\n",
    "            # Append PAD words so that all sentences are of the same length in each batch\n",
    "            # mark unseen word's tag as -1\n",
    "            # Maximum sentence lengths in current batch \n",
    "            batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "            # Intial matrix\n",
    "            ## Use -1 for initial tags to differentiate it with tags from PAD_WORDs\n",
    "            batch_data = self.word2idx[PAD_WORD]*np.ones((len(batch_sentences), batch_max_len))\n",
    "            batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "#             print(type(batch_data), type(batch_labels))\n",
    "\n",
    "\n",
    "            # Fill in the matrix with current batch sentences and labels\n",
    "            for i in range(len(batch_sentences)):\n",
    "                curr_len = len(batch_sentences[i])\n",
    "                batch_data[i][:curr_len] = batch_sentences[i]\n",
    "                batch_labels[i][:curr_len] = batch_tags[i]\n",
    "\n",
    "            # Convert to torch.LongTensors (since each entry is an index)\n",
    "#             batch_data = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "            batch_data, batch_labels = torch.from_numpy(batch_data), torch.from_numpy(batch_labels)\n",
    "            batch_data = batch_data.type(torch.LongTensor)\n",
    "            batch_labels = batch_labels.type(torch.LongTensor)\n",
    "#             print(type(batch_data), type(batch_labels))\n",
    "\n",
    "            # If gpu available\n",
    "            if params.cuda:\n",
    "                batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a data_iterator function using this logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data contains train_sentences and tarin_labels\n",
    "# params is a dictionary that contains a key of 'batch_size'\n",
    "loader = SentenceDataLoader('../data', 'word2idx.sav', 'tag2idx.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.vocab_size\n",
    "# train_iterator = data_iterator(train_data, dataiter_params, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train_sentences.sav \n",
      " ../data/train_labels.sav\n",
      "../data/dev_sentences.sav \n",
      " ../data/dev_labels.sav\n",
      "../data/testa_sentences.sav \n",
      " ../data/testa_labels.sav\n"
     ]
    }
   ],
   "source": [
    "d = loader.load_data(['train', 'dev', 'testa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = d['train']\n",
    "dev_data = d['dev']\n",
    "test_data = d['testa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model_utils.Params('../data/base_params.json')\n",
    "diter=loader.data_iterator(train_data, params)\n",
    "bdata, blabels = next(diter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "bdata=bdata.type(torch.LongTensor);print(bdata.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN (LSTM + FC)\n",
    "This model is largely taken from [cs230](https://github.com/cs230-stanford/cs230-code-examples/blob/master/pytorch/nlp/model/net.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        RNN model (lstm) that predicts the NER tags for each token in the sentence. It is composed of:\n",
    "        \n",
    "        - an embedding layer: maps each index in range(params.vocab_size) to a params.embedding_dim vector\n",
    "        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n",
    "        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n",
    "        \n",
    "        Args:\n",
    "            params (dict): contains vocab_size, embedding_dim, lstm_hidden_dim\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(params.vocab_size, params.embedding_dim)\n",
    "        self.lstm = nn.LSTM(params.embedding_dim, params.lstm_hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(params.lstm_hidden_dim, params.number_of_tags)\n",
    "        \n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (torch.tensor): a batch of sentences, of dimension batch_size x seq_len, where seq_len is\n",
    "               the length of the longest sentence in the batch. For sentences shorter than seq_len, the remaining\n",
    "               tokens are PAD_WORD. Each row is a sentence with each element corresponding to the index of\n",
    "               the token in the vocab.\n",
    "        Returns:\n",
    "            out (torch.tensor): of dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "        Note: the dimensions after each step are provided\n",
    "        \"\"\"\n",
    "        #                                -> batch_size x seq_len\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embed(s)            # dim: batch_size x seq_len x embedding_dim\n",
    "\n",
    "        # run the LSTM along the sentences of length seq_len\n",
    "        s, _ = self.lstm(s)              # dim: batch_size x seq_len x lstm_hidden_dim\n",
    "\n",
    "        # make the Variable contiguous in memory (a PyTorch artefact)\n",
    "        s = s.contiguous()\n",
    "\n",
    "        # reshape the Variable so that each row contains one token\n",
    "        s = s.view(-1, s.shape[2])       # dim: batch_size*seq_len x lstm_hidden_dim\n",
    "\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc(s)                   # dim: batch_size*seq_len x num_tags\n",
    "\n",
    "        # apply log softmax on each token's output (this is recommended over applying softmax\n",
    "        # since it is numerically more stable)\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*seq_len x num_tags\n",
    "\n",
    "\n",
    "def loss_fn(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss given outputs from the model and labels for all tokens. Exclude loss terms\n",
    "    for PADding tokens.\n",
    "    Args:\n",
    "        outputs: (Variable) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (Variable) dimension batch_size x seq_len where each element is either a label in [0, 1, ... num_tag-1],\n",
    "                or -1 in case it is a PADding token.\n",
    "    Returns:\n",
    "        loss: (Variable) cross entropy loss for all tokens in the batch\n",
    "    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n",
    "          demonstrates how you can easily define a custom loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    # indexing with negative values is not supported. Since PADded tokens have label -1, we convert them to a positive\n",
    "    # number. This does not affect training, since we ignore the PADded tokens with the mask.\n",
    "    labels = labels % outputs.shape[1]\n",
    "\n",
    "    num_tokens = int(torch.sum(mask).data[0])\n",
    "\n",
    "    # compute cross entropy loss for all tokens (except PADding tokens), by multiplying with mask.\n",
    "    return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy, given the outputs and labels for all tokens. Exclude PADding terms.\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size x seq_len where each element is either a label in\n",
    "                [0, 1, ... num_tag-1], or -1 in case it is a PADding token.\n",
    "    Returns: (float) accuracy in [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "#     print(f\"label shape (bs, seq_len): {labels.shape}\")\n",
    "    labels = labels.ravel()\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0)\n",
    "\n",
    "    # np.argmax gives us the class predicted for each token by the model\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    \n",
    "    # for debug\n",
    "#     print(f\"outputs size: {outputs.shape}\")\n",
    "#     print(f\"raveled label size: {labels.shape}\")\n",
    "#     pdb.set_trace()\n",
    "\n",
    "    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n",
    "    return np.sum(outputs==labels)/float(np.sum(mask))\n",
    "\n",
    "def f1_entity(outputs, labels, selected_tags=None):\n",
    "    \"\"\"\n",
    "    Compute entity-level F1 score per class first and aggregate over classes using either micro or macro.\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size x seq_len where each element is either a label in\n",
    "                [0, 1, ... num_tag-1], or -1 in case it is a PADding token.\n",
    "    Returns: (float) average f1 score over class f1 scores\n",
    "    \"\"\"\n",
    "    # Implement me!\n",
    "    labels = labels.ravel()\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    \n",
    "    # for debug\n",
    "#     print(f\"outputs size: {outputs.shape}\")\n",
    "#     print(f\"raveled label size: {labels.shape}\")\n",
    "#     pdb.set_trace()\n",
    "\n",
    "    if selected_tags is None:\n",
    "        selected_tags = [0,1,2,3,4,5,6,7,8] # To exclude -1 (PAD_TAG); todo: don't hard-code it though..\n",
    "        \n",
    "    return f1_score(outputs, labels, labels=selected_tags, average='weighted')\n",
    "\n",
    "    \n",
    "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "    'f1_entity':f1_entity,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  3  4  5]\n",
      "label shape (bs, seq_len): (5, 26)\n",
      "outputs size: (130,)\n",
      "raveled label size: (130,)\n",
      "> <ipython-input-97-b18f79c6f00f>(105)accuracy()\n",
      "-> return np.sum(outputs==labels)/float(np.sum(mask))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs size: (130,)\n",
      "raveled label size: (130,)\n",
      "> <ipython-input-97-b18f79c6f00f>(122)f1_entity()\n",
      "-> if selected_tags is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 1.0\n",
      "f1: 0.6211985688729875\n",
      "> <ipython-input-99-34c0270796e8>(6)<module>()\n",
      "-> for bdata, blabels in diter:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1  3  4  5  6]\n",
      "label shape (bs, seq_len): (5, 34)\n",
      "outputs size: (170,)\n",
      "raveled label size: (170,)\n",
      "> <ipython-input-97-b18f79c6f00f>(105)accuracy()\n",
      "-> return np.sum(outputs==labels)/float(np.sum(mask))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs size: (170,)\n",
      "raveled label size: (170,)\n",
      "> <ipython-input-97-b18f79c6f00f>(122)f1_entity()\n",
      "-> if selected_tags is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 1.0\n",
      "f1: 0.4363407430708469\n",
      "> <ipython-input-99-34c0270796e8>(6)<module>()\n",
      "-> for bdata, blabels in diter:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1  3  4  5  6]\n",
      "label shape (bs, seq_len): (5, 19)\n",
      "outputs size: (95,)\n",
      "raveled label size: (95,)\n",
      "> <ipython-input-97-b18f79c6f00f>(105)accuracy()\n",
      "-> return np.sum(outputs==labels)/float(np.sum(mask))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs size: (95,)\n",
      "raveled label size: (95,)\n",
      "> <ipython-input-97-b18f79c6f00f>(122)f1_entity()\n",
      "-> if selected_tags is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 1.0\n",
      "f1: 0.5780205230056273\n",
      "> <ipython-input-99-34c0270796e8>(6)<module>()\n",
      "-> for bdata, blabels in diter:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1  2  3  6]\n",
      "label shape (bs, seq_len): (5, 24)\n",
      "outputs size: (120,)\n",
      "raveled label size: (120,)\n",
      "> <ipython-input-97-b18f79c6f00f>(105)accuracy()\n",
      "-> return np.sum(outputs==labels)/float(np.sum(mask))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs size: (120,)\n",
      "raveled label size: (120,)\n",
      "> <ipython-input-97-b18f79c6f00f>(122)f1_entity()\n",
      "-> if selected_tags is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 1.0\n",
      "f1: 0.6915708812260536\n",
      "> <ipython-input-99-34c0270796e8>(6)<module>()\n",
      "-> for bdata, blabels in diter:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1  2  3  4]\n",
      "label shape (bs, seq_len): (5, 26)\n",
      "outputs size: (130,)\n",
      "raveled label size: (130,)\n",
      "> <ipython-input-97-b18f79c6f00f>(105)accuracy()\n",
      "-> return np.sum(outputs==labels)/float(np.sum(mask))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs size: (130,)\n",
      "raveled label size: (130,)\n",
      "> <ipython-input-97-b18f79c6f00f>(122)f1_entity()\n",
      "-> if selected_tags is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9787234042553191\n",
      "f1: 0.5113859136643947\n",
      "> <ipython-input-99-34c0270796e8>(6)<module>()\n",
      "-> for bdata, blabels in diter:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-34c0270796e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mditer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mditer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-34c0270796e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mditer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mditer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_metrics():\n",
    "    # test metric functions\n",
    "    tag_set = np.unique([t for tlist in train_labels for t in tlist])\n",
    "    params = model_utils.Params('../data/base_params.json')\n",
    "    diter=loader.data_iterator(train_data, params)\n",
    "    with torch.no_grad():\n",
    "        for bdata, blabels in diter:\n",
    "            output = test_model(bdata)\n",
    "            print(np.unique(blabels))\n",
    "            acc = accuracy(output.numpy(), blabels.numpy())\n",
    "            f1 = f1_entity(output.numpy(), blabels.numpy())\n",
    "            print(f\"acc: {acc}\")\n",
    "            print(f\"f1: {f1}\")\n",
    "            pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training \n",
    "def train(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = model_utils.RunningAverage()\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps) \n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "\n",
    "        train_batch, labels_batch = next(data_iterator)        \n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % params.save_summary_steps == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.data[0]\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.data[0])\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "    \n",
    "\n",
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_data: (dict) training data with keys 'data' and 'labels'\n",
    "        val_data: (dict) validaion data with keys 'data' and 'labels'\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "#     if restore_file is not None:\n",
    "#         restore_path = os.path.join(model_dir, restore_file + '.pth.tar')\n",
    "#         logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "#         nlp_utils.load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (params.train_size + 1) // params.batch_size\n",
    "        train_data_iterator = data_loader.data_iterator(train_data, params, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, metrics, params, num_steps)\n",
    "            \n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (params.val_size + 1) // params.batch_size\n",
    "        val_data_iterator = data_loader.data_iterator(val_data, params, shuffle=False)\n",
    "        val_metrics = evaluate(model, loss_fn, val_data_iterator, metrics, params, num_steps)\n",
    "        \n",
    "        val_acc = val_metrics['f1_entity']\n",
    "        is_best = val_acc >= best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        model_utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()}, \n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "            \n",
    "        # If best_eval, best_save_path        \n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best f1_entity\")\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            model_utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        model_utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train this LSTM + FC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n"
     ]
    }
   ],
   "source": [
    "# Set params\n",
    "params = model_utils.Params('../data/rnn_1_params_2.json')\n",
    "\n",
    "# set a directory to save our model\n",
    "model_dir = '../log/progress_rnn1_10_09_12_12'\n",
    "data_dir = '../data'\n",
    "\n",
    "# use GPU if available\n",
    "# params.cuda = torch.cuda.is_available()\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "# if params.cuda: torch.cuda.manual_seed(230)\n",
    "\n",
    "# Set the logger\n",
    "model_utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "# load data\n",
    "data_loader = SentenceDataLoader(data_dir, 'word2idx.sav', 'tag2idx.sav')\n",
    "# data = data_loader.load_data(datadir, ['train', 'val'])\n",
    "# train_data = data['train']\n",
    "# val_data = data['val']\n",
    "\n",
    "# specify the train and val dataset sizes\n",
    "params.train_size = train_data['size']\n",
    "params.val_size = dev_data['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting training for 200 epoch(s)\n",
      "Epoch 1/200\n",
      "\n",
      "\n",
      "  0%|          | 0/3497 [00:00<?, ?it/s]\u001b[A\u001b[A/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "\n",
      "\n",
      "  0%|          | 0/3497 [00:00<?, ?it/s, loss=2.404]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/3497 [00:00<?, ?it/s, loss=2.358]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 2/3497 [00:00<03:59, 14.58it/s, loss=2.358]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 2/3497 [00:00<03:59, 14.58it/s, loss=2.319]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 2/3497 [00:00<03:59, 14.58it/s, loss=2.300]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/3497 [00:00<03:42, 15.71it/s, loss=2.300]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/3497 [00:00<03:42, 15.71it/s, loss=2.282]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/3497 [00:00<03:42, 15.71it/s, loss=2.234]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/3497 [00:00<03:42, 15.71it/s, loss=2.210]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 7/3497 [00:00<03:23, 17.17it/s, loss=2.210]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 7/3497 [00:00<03:23, 17.17it/s, loss=2.167]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 7/3497 [00:00<03:23, 17.17it/s, loss=2.142]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 9/3497 [00:00<03:17, 17.62it/s, loss=2.142]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 9/3497 [00:00<03:17, 17.62it/s, loss=2.117]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 9/3497 [00:00<03:17, 17.62it/s, loss=2.085]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 11/3497 [00:00<03:20, 17.41it/s, loss=2.085]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 11/3497 [00:00<03:20, 17.41it/s, loss=2.045]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 11/3497 [00:00<03:20, 17.41it/s, loss=2.011]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 13/3497 [00:00<03:14, 17.87it/s, loss=2.011]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 13/3497 [00:00<03:14, 17.87it/s, loss=1.965]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 13/3497 [00:00<03:14, 17.87it/s, loss=1.956]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 15/3497 [00:00<03:09, 18.40it/s, loss=1.956]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 15/3497 [00:00<03:09, 18.40it/s, loss=1.930]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 15/3497 [00:00<03:09, 18.40it/s, loss=1.838]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 15/3497 [00:00<03:09, 18.40it/s, loss=1.807]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 18/3497 [00:00<02:56, 19.66it/s, loss=1.807]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 18/3497 [00:00<02:56, 19.66it/s, loss=1.774]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 18/3497 [00:01<02:56, 19.66it/s, loss=1.717]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 20/3497 [00:01<02:56, 19.71it/s, loss=1.717]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 20/3497 [00:01<02:56, 19.71it/s, loss=1.687]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 20/3497 [00:01<02:56, 19.71it/s, loss=1.674]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 20/3497 [00:01<02:56, 19.71it/s, loss=1.672]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 23/3497 [00:01<02:48, 20.59it/s, loss=1.672]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 23/3497 [00:01<02:48, 20.59it/s, loss=1.681]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 23/3497 [00:01<02:48, 20.59it/s, loss=1.685]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 23/3497 [00:01<02:48, 20.59it/s, loss=1.662]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 26/3497 [00:01<02:57, 19.58it/s, loss=1.662]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 26/3497 [00:01<02:57, 19.58it/s, loss=1.628]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 26/3497 [00:01<02:57, 19.58it/s, loss=1.595]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 26/3497 [00:01<02:57, 19.58it/s, loss=1.573]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 29/3497 [00:01<03:01, 19.06it/s, loss=1.573]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 29/3497 [00:01<03:01, 19.06it/s, loss=1.548]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 29/3497 [00:01<03:01, 19.06it/s, loss=1.531]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 31/3497 [00:01<03:03, 18.87it/s, loss=1.531]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 31/3497 [00:01<03:03, 18.87it/s, loss=1.508]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 31/3497 [00:01<03:03, 18.87it/s, loss=1.483]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 33/3497 [00:01<03:01, 19.12it/s, loss=1.483]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 33/3497 [00:01<03:01, 19.12it/s, loss=1.470]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 33/3497 [00:01<03:01, 19.12it/s, loss=1.457]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 35/3497 [00:01<03:07, 18.44it/s, loss=1.457]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 35/3497 [00:01<03:07, 18.44it/s, loss=1.466]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 35/3497 [00:01<03:07, 18.44it/s, loss=1.475]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 35/3497 [00:01<03:07, 18.44it/s, loss=1.469]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 38/3497 [00:01<02:57, 19.50it/s, loss=1.469]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 38/3497 [00:01<02:57, 19.50it/s, loss=1.465]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 38/3497 [00:02<02:57, 19.50it/s, loss=1.437]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 38/3497 [00:02<02:57, 19.50it/s, loss=1.420]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 41/3497 [00:02<02:50, 20.22it/s, loss=1.420]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 41/3497 [00:02<02:50, 20.22it/s, loss=1.413]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 41/3497 [00:02<02:50, 20.22it/s, loss=1.399]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 41/3497 [00:02<02:50, 20.22it/s, loss=1.378]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 44/3497 [00:02<02:40, 21.49it/s, loss=1.378]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 44/3497 [00:02<02:40, 21.49it/s, loss=1.374]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 44/3497 [00:02<02:40, 21.49it/s, loss=1.364]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 44/3497 [00:02<02:40, 21.49it/s, loss=1.355]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 47/3497 [00:02<02:32, 22.62it/s, loss=1.355]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 47/3497 [00:02<02:32, 22.62it/s, loss=1.333]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 47/3497 [00:02<02:32, 22.62it/s, loss=1.313]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 47/3497 [00:02<02:32, 22.62it/s, loss=1.308]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 50/3497 [00:02<02:45, 20.82it/s, loss=1.308]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 50/3497 [00:02<02:45, 20.82it/s, loss=1.299]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 50/3497 [00:02<02:45, 20.82it/s, loss=1.281]\u001b[A\u001b[A\n",
      "\n",
      "  1%|         | 50/3497 [00:02<02:45, 20.82it/s, loss=1.271]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 53/3497 [00:02<02:53, 19.89it/s, loss=1.271]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 53/3497 [00:02<02:53, 19.89it/s, loss=1.261]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 53/3497 [00:02<02:53, 19.89it/s, loss=1.254]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 53/3497 [00:02<02:53, 19.89it/s, loss=1.250]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 56/3497 [00:02<02:50, 20.17it/s, loss=1.250]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 56/3497 [00:02<02:50, 20.17it/s, loss=1.237]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 56/3497 [00:02<02:50, 20.17it/s, loss=1.225]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 56/3497 [00:02<02:50, 20.17it/s, loss=1.217]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 59/3497 [00:02<02:49, 20.28it/s, loss=1.217]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 59/3497 [00:03<02:49, 20.28it/s, loss=1.201]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 59/3497 [00:03<02:49, 20.28it/s, loss=1.206]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 59/3497 [00:03<02:49, 20.28it/s, loss=1.203]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 62/3497 [00:03<02:54, 19.64it/s, loss=1.203]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 62/3497 [00:03<02:54, 19.64it/s, loss=1.197]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 62/3497 [00:03<02:54, 19.64it/s, loss=1.194]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 62/3497 [00:03<02:54, 19.64it/s, loss=1.198]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 65/3497 [00:03<02:41, 21.22it/s, loss=1.198]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 65/3497 [00:03<02:41, 21.22it/s, loss=1.191]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 65/3497 [00:03<02:41, 21.22it/s, loss=1.191]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 65/3497 [00:03<02:41, 21.22it/s, loss=1.179]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 68/3497 [00:03<02:45, 20.73it/s, loss=1.179]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 68/3497 [00:03<02:45, 20.73it/s, loss=1.180]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 68/3497 [00:03<02:45, 20.73it/s, loss=1.171]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 68/3497 [00:03<02:45, 20.73it/s, loss=1.170]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 71/3497 [00:03<02:41, 21.27it/s, loss=1.170]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 71/3497 [00:03<02:41, 21.27it/s, loss=1.173]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 71/3497 [00:03<02:41, 21.27it/s, loss=1.165]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 71/3497 [00:03<02:41, 21.27it/s, loss=1.159]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 74/3497 [00:03<02:36, 21.85it/s, loss=1.159]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 74/3497 [00:03<02:36, 21.85it/s, loss=1.149]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 74/3497 [00:03<02:36, 21.85it/s, loss=1.143]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 74/3497 [00:03<02:36, 21.85it/s, loss=1.136]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 77/3497 [00:03<02:39, 21.50it/s, loss=1.136]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 77/3497 [00:03<02:39, 21.50it/s, loss=1.131]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 77/3497 [00:03<02:39, 21.50it/s, loss=1.123]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 77/3497 [00:03<02:39, 21.50it/s, loss=1.112]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 80/3497 [00:03<02:51, 19.94it/s, loss=1.112]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 80/3497 [00:04<02:51, 19.94it/s, loss=1.113]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 80/3497 [00:04<02:51, 19.94it/s, loss=1.109]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 80/3497 [00:04<02:51, 19.94it/s, loss=1.099]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 83/3497 [00:04<02:57, 19.26it/s, loss=1.099]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 83/3497 [00:04<02:57, 19.26it/s, loss=1.094]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 83/3497 [00:04<02:57, 19.26it/s, loss=1.087]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 83/3497 [00:04<02:57, 19.26it/s, loss=1.082]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 86/3497 [00:04<02:49, 20.13it/s, loss=1.082]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 86/3497 [00:04<02:49, 20.13it/s, loss=1.079]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 86/3497 [00:04<02:49, 20.13it/s, loss=1.074]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 86/3497 [00:04<02:49, 20.13it/s, loss=1.068]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 89/3497 [00:04<02:39, 21.43it/s, loss=1.068]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 89/3497 [00:04<02:39, 21.43it/s, loss=1.059]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 89/3497 [00:04<02:39, 21.43it/s, loss=1.057]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 89/3497 [00:04<02:39, 21.43it/s, loss=1.050]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 92/3497 [00:04<02:46, 20.50it/s, loss=1.050]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 92/3497 [00:04<02:46, 20.50it/s, loss=1.050]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 92/3497 [00:04<02:46, 20.50it/s, loss=1.056]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 92/3497 [00:04<02:46, 20.50it/s, loss=1.048]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 95/3497 [00:04<02:39, 21.38it/s, loss=1.048]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 95/3497 [00:04<02:39, 21.38it/s, loss=1.047]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 95/3497 [00:04<02:39, 21.38it/s, loss=1.043]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 95/3497 [00:04<02:39, 21.38it/s, loss=1.040]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 98/3497 [00:04<02:36, 21.67it/s, loss=1.040]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 98/3497 [00:04<02:36, 21.67it/s, loss=1.037]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 98/3497 [00:04<02:36, 21.67it/s, loss=1.031]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 98/3497 [00:04<02:36, 21.67it/s, loss=1.031]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 101/3497 [00:04<02:35, 21.77it/s, loss=1.031]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 101/3497 [00:05<02:35, 21.77it/s, loss=1.027]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 101/3497 [00:05<02:35, 21.77it/s, loss=1.027]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 101/3497 [00:05<02:35, 21.77it/s, loss=1.025]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 104/3497 [00:05<02:37, 21.50it/s, loss=1.025]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 104/3497 [00:05<02:37, 21.50it/s, loss=1.028]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 104/3497 [00:05<02:37, 21.50it/s, loss=1.027]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 104/3497 [00:05<02:37, 21.50it/s, loss=1.024]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 107/3497 [00:05<02:41, 21.02it/s, loss=1.024]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 107/3497 [00:05<02:41, 21.02it/s, loss=1.019]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 107/3497 [00:05<02:41, 21.02it/s, loss=1.022]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 107/3497 [00:05<02:41, 21.02it/s, loss=1.015]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 110/3497 [00:05<02:33, 22.07it/s, loss=1.015]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 110/3497 [00:05<02:33, 22.07it/s, loss=1.013]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 110/3497 [00:05<02:33, 22.07it/s, loss=1.008]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 110/3497 [00:05<02:33, 22.07it/s, loss=1.007]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 113/3497 [00:05<02:27, 22.95it/s, loss=1.007]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 113/3497 [00:05<02:27, 22.95it/s, loss=1.004]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 113/3497 [00:05<02:27, 22.95it/s, loss=1.007]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 113/3497 [00:05<02:27, 22.95it/s, loss=1.005]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 116/3497 [00:05<02:28, 22.75it/s, loss=1.005]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 116/3497 [00:05<02:28, 22.75it/s, loss=1.004]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 116/3497 [00:05<02:28, 22.75it/s, loss=0.999]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 116/3497 [00:05<02:28, 22.75it/s, loss=1.001]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 119/3497 [00:05<02:28, 22.78it/s, loss=1.001]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 119/3497 [00:05<02:28, 22.78it/s, loss=1.002]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 119/3497 [00:05<02:28, 22.78it/s, loss=1.007]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 119/3497 [00:05<02:28, 22.78it/s, loss=1.002]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 122/3497 [00:05<02:32, 22.13it/s, loss=1.002]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 122/3497 [00:05<02:32, 22.13it/s, loss=1.000]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 122/3497 [00:05<02:32, 22.13it/s, loss=0.999]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 122/3497 [00:06<02:32, 22.13it/s, loss=0.993]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 125/3497 [00:06<02:37, 21.43it/s, loss=0.993]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 125/3497 [00:06<02:37, 21.43it/s, loss=0.994]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 125/3497 [00:06<02:37, 21.43it/s, loss=0.988]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 125/3497 [00:06<02:37, 21.43it/s, loss=0.989]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 128/3497 [00:06<02:49, 19.87it/s, loss=0.989]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 128/3497 [00:06<02:49, 19.87it/s, loss=0.987]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 128/3497 [00:06<02:49, 19.87it/s, loss=0.982]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 128/3497 [00:06<02:49, 19.87it/s, loss=0.983]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 131/3497 [00:06<02:41, 20.79it/s, loss=0.983]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 131/3497 [00:06<02:41, 20.79it/s, loss=0.980]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 131/3497 [00:06<02:41, 20.79it/s, loss=0.981]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 131/3497 [00:06<02:41, 20.79it/s, loss=0.984]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 134/3497 [00:06<02:35, 21.62it/s, loss=0.984]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 134/3497 [00:06<02:35, 21.62it/s, loss=0.986]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 134/3497 [00:06<02:35, 21.62it/s, loss=0.984]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 134/3497 [00:06<02:35, 21.62it/s, loss=0.988]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 137/3497 [00:06<02:27, 22.71it/s, loss=0.988]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 137/3497 [00:06<02:27, 22.71it/s, loss=0.984]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 137/3497 [00:06<02:27, 22.71it/s, loss=0.984]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 137/3497 [00:06<02:27, 22.71it/s, loss=0.978]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 140/3497 [00:06<02:32, 22.06it/s, loss=0.978]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 140/3497 [00:06<02:32, 22.06it/s, loss=0.982]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 140/3497 [00:06<02:32, 22.06it/s, loss=0.985]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 140/3497 [00:06<02:32, 22.06it/s, loss=0.982]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 143/3497 [00:06<02:30, 22.21it/s, loss=0.982]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 143/3497 [00:06<02:30, 22.21it/s, loss=0.979]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 143/3497 [00:06<02:30, 22.21it/s, loss=0.975]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 143/3497 [00:07<02:30, 22.21it/s, loss=0.972]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 146/3497 [00:07<02:41, 20.72it/s, loss=0.972]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 146/3497 [00:07<02:41, 20.72it/s, loss=0.971]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 146/3497 [00:07<02:41, 20.72it/s, loss=0.973]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 146/3497 [00:07<02:41, 20.72it/s, loss=0.969]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 149/3497 [00:07<02:42, 20.66it/s, loss=0.969]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 149/3497 [00:07<02:42, 20.66it/s, loss=0.968]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 149/3497 [00:07<02:42, 20.66it/s, loss=0.972]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 149/3497 [00:07<02:42, 20.66it/s, loss=0.970]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 152/3497 [00:07<02:40, 20.83it/s, loss=0.970]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 152/3497 [00:07<02:40, 20.83it/s, loss=0.968]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 152/3497 [00:07<02:40, 20.83it/s, loss=0.965]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 152/3497 [00:07<02:40, 20.83it/s, loss=0.965]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 155/3497 [00:07<02:38, 21.14it/s, loss=0.965]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 155/3497 [00:07<02:38, 21.14it/s, loss=0.961]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 155/3497 [00:07<02:38, 21.14it/s, loss=0.960]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 155/3497 [00:07<02:38, 21.14it/s, loss=0.959]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 158/3497 [00:07<02:33, 21.76it/s, loss=0.959]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 158/3497 [00:07<02:33, 21.76it/s, loss=0.957]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 158/3497 [00:07<02:33, 21.76it/s, loss=0.956]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 158/3497 [00:07<02:33, 21.76it/s, loss=0.954]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 161/3497 [00:07<02:33, 21.76it/s, loss=0.954]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 161/3497 [00:07<02:33, 21.76it/s, loss=0.950]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 161/3497 [00:07<02:33, 21.76it/s, loss=0.948]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 161/3497 [00:07<02:33, 21.76it/s, loss=0.945]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 164/3497 [00:07<02:45, 20.19it/s, loss=0.945]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 164/3497 [00:07<02:45, 20.19it/s, loss=0.944]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 164/3497 [00:07<02:45, 20.19it/s, loss=0.941]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 164/3497 [00:08<02:45, 20.19it/s, loss=0.941]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 167/3497 [00:08<02:37, 21.16it/s, loss=0.941]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 167/3497 [00:08<02:37, 21.16it/s, loss=0.942]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 167/3497 [00:08<02:37, 21.16it/s, loss=0.940]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 167/3497 [00:08<02:37, 21.16it/s, loss=0.937]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 170/3497 [00:08<02:35, 21.46it/s, loss=0.937]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 170/3497 [00:08<02:35, 21.46it/s, loss=0.936]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 170/3497 [00:08<02:35, 21.46it/s, loss=0.932]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 170/3497 [00:08<02:35, 21.46it/s, loss=0.929]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 173/3497 [00:08<02:29, 22.20it/s, loss=0.929]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 173/3497 [00:08<02:29, 22.20it/s, loss=0.927]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 173/3497 [00:08<02:29, 22.20it/s, loss=0.924]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 173/3497 [00:08<02:29, 22.20it/s, loss=0.923]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 176/3497 [00:08<02:34, 21.48it/s, loss=0.923]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 176/3497 [00:08<02:34, 21.48it/s, loss=0.922]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 176/3497 [00:08<02:34, 21.48it/s, loss=0.925]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 176/3497 [00:08<02:34, 21.48it/s, loss=0.924]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 179/3497 [00:08<02:43, 20.31it/s, loss=0.924]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 179/3497 [00:08<02:43, 20.31it/s, loss=0.920]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 179/3497 [00:08<02:43, 20.31it/s, loss=0.920]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 179/3497 [00:08<02:43, 20.31it/s, loss=0.917]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 182/3497 [00:08<02:38, 20.91it/s, loss=0.917]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 182/3497 [00:08<02:38, 20.91it/s, loss=0.916]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 182/3497 [00:08<02:38, 20.91it/s, loss=0.914]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 182/3497 [00:08<02:38, 20.91it/s, loss=0.916]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 185/3497 [00:08<02:29, 22.16it/s, loss=0.916]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 185/3497 [00:08<02:29, 22.16it/s, loss=0.916]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 185/3497 [00:08<02:29, 22.16it/s, loss=0.913]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 185/3497 [00:08<02:29, 22.16it/s, loss=0.913]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 188/3497 [00:09<02:31, 21.90it/s, loss=0.913]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 188/3497 [00:09<02:31, 21.90it/s, loss=0.909]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 188/3497 [00:09<02:31, 21.90it/s, loss=0.907]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 188/3497 [00:09<02:31, 21.90it/s, loss=0.907]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 191/3497 [00:09<02:31, 21.79it/s, loss=0.907]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 191/3497 [00:09<02:31, 21.79it/s, loss=0.904]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 191/3497 [00:09<02:31, 21.79it/s, loss=0.902]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 191/3497 [00:09<02:31, 21.79it/s, loss=0.900]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 194/3497 [00:09<02:42, 20.29it/s, loss=0.900]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 194/3497 [00:09<02:42, 20.29it/s, loss=0.900]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 194/3497 [00:09<02:42, 20.29it/s, loss=0.896]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 194/3497 [00:09<02:42, 20.29it/s, loss=0.894]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 197/3497 [00:09<02:42, 20.33it/s, loss=0.894]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 197/3497 [00:09<02:42, 20.33it/s, loss=0.892]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 197/3497 [00:09<02:42, 20.33it/s, loss=0.889]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 197/3497 [00:09<02:42, 20.33it/s, loss=0.887]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 200/3497 [00:09<02:40, 20.54it/s, loss=0.887]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 200/3497 [00:09<02:40, 20.54it/s, loss=0.885]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 200/3497 [00:09<02:40, 20.54it/s, loss=0.885]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 200/3497 [00:09<02:40, 20.54it/s, loss=0.888]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 203/3497 [00:09<02:30, 21.90it/s, loss=0.888]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 203/3497 [00:09<02:30, 21.90it/s, loss=0.886]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 203/3497 [00:09<02:30, 21.90it/s, loss=0.888]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 203/3497 [00:09<02:30, 21.90it/s, loss=0.885]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 206/3497 [00:09<02:26, 22.40it/s, loss=0.885]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 206/3497 [00:09<02:26, 22.40it/s, loss=0.889]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 206/3497 [00:09<02:26, 22.40it/s, loss=0.889]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 206/3497 [00:09<02:26, 22.40it/s, loss=0.886]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 209/3497 [00:09<02:22, 23.07it/s, loss=0.886]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 209/3497 [00:10<02:22, 23.07it/s, loss=0.883]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 209/3497 [00:10<02:22, 23.07it/s, loss=0.882]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 209/3497 [00:10<02:22, 23.07it/s, loss=0.880]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 212/3497 [00:10<02:31, 21.62it/s, loss=0.880]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 212/3497 [00:10<02:31, 21.62it/s, loss=0.878]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 212/3497 [00:10<02:31, 21.62it/s, loss=0.875]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 212/3497 [00:10<02:31, 21.62it/s, loss=0.874]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 215/3497 [00:10<02:35, 21.10it/s, loss=0.874]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 215/3497 [00:10<02:35, 21.10it/s, loss=0.872]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 215/3497 [00:10<02:35, 21.10it/s, loss=0.871]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 215/3497 [00:10<02:35, 21.10it/s, loss=0.869]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 218/3497 [00:10<02:38, 20.63it/s, loss=0.869]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 218/3497 [00:10<02:38, 20.63it/s, loss=0.871]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 218/3497 [00:10<02:38, 20.63it/s, loss=0.872]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 218/3497 [00:10<02:38, 20.63it/s, loss=0.873]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 221/3497 [00:10<02:25, 22.48it/s, loss=0.873]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 221/3497 [00:10<02:25, 22.48it/s, loss=0.872]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 221/3497 [00:10<02:25, 22.48it/s, loss=0.871]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 221/3497 [00:10<02:25, 22.48it/s, loss=0.872]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 224/3497 [00:10<02:41, 20.29it/s, loss=0.872]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 224/3497 [00:10<02:41, 20.29it/s, loss=0.871]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 224/3497 [00:10<02:41, 20.29it/s, loss=0.870]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 224/3497 [00:10<02:41, 20.29it/s, loss=0.868]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 227/3497 [00:10<02:43, 19.97it/s, loss=0.868]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 227/3497 [00:10<02:43, 19.97it/s, loss=0.868]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 227/3497 [00:10<02:43, 19.97it/s, loss=0.867]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 227/3497 [00:11<02:43, 19.97it/s, loss=0.865]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 230/3497 [00:11<02:52, 18.97it/s, loss=0.865]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 230/3497 [00:11<02:52, 18.97it/s, loss=0.867]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 230/3497 [00:11<02:52, 18.97it/s, loss=0.866]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 232/3497 [00:11<02:52, 18.93it/s, loss=0.866]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 232/3497 [00:11<02:52, 18.93it/s, loss=0.864]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 232/3497 [00:11<02:52, 18.93it/s, loss=0.861]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 234/3497 [00:11<03:02, 17.91it/s, loss=0.861]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 234/3497 [00:11<03:02, 17.91it/s, loss=0.861]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 234/3497 [00:11<03:02, 17.91it/s, loss=0.859]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 236/3497 [00:11<03:03, 17.78it/s, loss=0.859]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 236/3497 [00:11<03:03, 17.78it/s, loss=0.859]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 236/3497 [00:11<03:03, 17.78it/s, loss=0.857]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 238/3497 [00:11<03:01, 17.98it/s, loss=0.857]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 238/3497 [00:11<03:01, 17.98it/s, loss=0.858]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 238/3497 [00:11<03:01, 17.98it/s, loss=0.857]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 238/3497 [00:11<03:01, 17.98it/s, loss=0.855]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 241/3497 [00:11<02:55, 18.54it/s, loss=0.855]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 241/3497 [00:11<02:55, 18.54it/s, loss=0.852]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 241/3497 [00:11<02:55, 18.54it/s, loss=0.852]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 241/3497 [00:11<02:55, 18.54it/s, loss=0.852]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 244/3497 [00:11<02:51, 18.93it/s, loss=0.852]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 244/3497 [00:11<02:51, 18.93it/s, loss=0.850]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 244/3497 [00:11<02:51, 18.93it/s, loss=0.848]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 244/3497 [00:11<02:51, 18.93it/s, loss=0.849]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 247/3497 [00:11<02:41, 20.17it/s, loss=0.849]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 247/3497 [00:11<02:41, 20.17it/s, loss=0.849]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 247/3497 [00:12<02:41, 20.17it/s, loss=0.850]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 247/3497 [00:12<02:41, 20.17it/s, loss=0.850]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 250/3497 [00:12<02:46, 19.54it/s, loss=0.850]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 250/3497 [00:12<02:46, 19.54it/s, loss=0.848]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 250/3497 [00:12<02:46, 19.54it/s, loss=0.846]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 250/3497 [00:12<02:46, 19.54it/s, loss=0.845]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 253/3497 [00:12<02:41, 20.15it/s, loss=0.845]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 253/3497 [00:12<02:41, 20.15it/s, loss=0.845]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 253/3497 [00:12<02:41, 20.15it/s, loss=0.845]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 253/3497 [00:12<02:41, 20.15it/s, loss=0.847]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 256/3497 [00:12<02:33, 21.13it/s, loss=0.847]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 256/3497 [00:12<02:33, 21.13it/s, loss=0.848]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 256/3497 [00:12<02:33, 21.13it/s, loss=0.847]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 256/3497 [00:12<02:33, 21.13it/s, loss=0.847]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 259/3497 [00:12<02:34, 20.91it/s, loss=0.847]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 259/3497 [00:12<02:34, 20.91it/s, loss=0.846]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 259/3497 [00:12<02:34, 20.91it/s, loss=0.846]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 259/3497 [00:12<02:34, 20.91it/s, loss=0.844]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 262/3497 [00:12<02:42, 19.93it/s, loss=0.844]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 262/3497 [00:12<02:42, 19.93it/s, loss=0.843]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 262/3497 [00:12<02:42, 19.93it/s, loss=0.841]\u001b[A\u001b[A\n",
      "\n",
      "  7%|         | 262/3497 [00:12<02:42, 19.93it/s, loss=0.840]\u001b[A\u001b[A\n",
      "\n",
      "  8%|         | 265/3497 [00:12<02:44, 19.68it/s, loss=0.840]\u001b[A\u001b[A\n",
      "\n",
      "  8%|         | 265/3497 [00:12<02:44, 19.68it/s, loss=0.839]\u001b[A\u001b[A\n",
      "\n",
      "  8%|         | 265/3497 [00:12<02:44, 19.68it/s, loss=0.837]\u001b[A\u001b[A\n",
      "\n",
      "  8%|         | 265/3497 [00:12<02:44, 19.68it/s, loss=0.836]\u001b[A\u001b[A\n",
      "\n",
      "  8%|         | 268/3497 [00:12<02:36, 20.60it/s, loss=0.836]\u001b[A\u001b[A\n",
      "\n",
      "  8%|         | 268/3497 [00:13<02:36, 20.60it/s, loss=0.836]\u001b[A\u001b[A\n",
      "\n",
      "  8%|         | 268/3497 [00:13<02:36, 20.60it/s, loss=0.834]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "# Define the model and optimizer\n",
    "model = Net(params).cuda() if params.cuda else Net(params)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "# fetch loss function and metrics\n",
    "loss_fn = loss_fn\n",
    "metrics = metrics\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "train_and_evaluate(model, train_data, dev_data, optimizer, loss_fn, metrics, params, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on `testb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 70\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model_dir = '../log/progress_rnn1'\n",
    "# data_dir = '../data'\n",
    "params = model_utils.Params('../data/rnn_1_params_1-Copy1.json')\n",
    "old_model = Net(params)\n",
    "new_model = Net(params)\n",
    "old_checkpoint = model_utils.load_checkpoint('../log/progress_rnn1/old_best.pth.tar', old_model);\n",
    "new_checkpoint = model_utils.load_checkpoint('../log/progress_rnn1/best.pth.tar', new_model);\n",
    "print(old_checkpoint['epoch'], new_checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embed): Embedding(15002, 500)\n",
       "  (lstm): LSTM(500, 200, batch_first=True)\n",
       "  (fc): Linear(in_features=200, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-20-a703289ccb45>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-a703289ccb45>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    model_dir = '../log/\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "\"\"\"\n",
    "# Load the parameters\n",
    "# model_dir = '../log/\n",
    "# json_path = os.path.join(model_dir, 'rnn1_params_1.json')\n",
    "# assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "# params = model_utils.Params(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testb dataset\n",
    "testb_sentences = joblib.load('../data/testb_sentences.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse dictionary\n",
    "idx2tag = {i:t for t,i in tag2idx.items()}\n",
    "idx2word = {i:w for w,i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = old_model\n",
    "test_model = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "3683\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    for i in range(len(testb_sentences)):\n",
    "        prob = best_model(torch.LongTensor([test_sentences[i]])).numpy()\n",
    "        tags_i = np.argmax(prob, axis=1)\n",
    "        tags = [idx2tag[idx] for idx in tags_i]\n",
    "        test_preds.append(tags)\n",
    "        \n",
    "        # debugging\n",
    "#         print(\"=\"*50)\n",
    "#         print(i, \"\\n\", prob.shape)\n",
    "#         print(\"number of words: \", len(tags))\n",
    "#         print(len(tags), \"\\n\", tags)\n",
    "#         words = [idx2word[j] for j in test_sentences[i]]\n",
    "#         print(words)\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "\n",
    "print(\"done\")\n",
    "print(len(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  3  4  5]\n",
      "label shape (bs, seq_len): (5, 26)\n",
      "outputs size: (130,)\n",
      "raveled label size: (130,)\n",
      "> <ipython-input-88-a39a786fd046>(105)accuracy()\n",
      "-> return np.sum(outputs==labels)/float(np.sum(mask))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  \t    print(f\"outputs size: {outputs.shape}\")\n",
      "101  \t    print(f\"raveled label size: {labels.shape}\")\n",
      "102  \t    pdb.set_trace()\n",
      "103  \t\n",
      "104  \t    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n",
      "105  ->\t    return np.sum(outputs==labels)/float(np.sum(mask))\n",
      "106  \t\n",
      "107  \tdef f1_entity(outputs, labels, selected_tags=None):\n",
      "108  \t    \"\"\"\n",
      "109  \t    Compute entity-level F1 score per class first and aggregate over classes using either micro or macro.\n",
      "110  \t    Args:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs size: (130,)\n",
      "raveled label size: (130,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [130, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-27828d9335de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_entity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"acc: {acc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"f1: {f1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-a39a786fd046>\u001b[0m in \u001b[0;36mf1_entity\u001b[0;34m(outputs, labels, selected_tags)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mselected_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# To exclude -1 (PAD_TAG); todo: don't hard-code it though..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    718\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m    719\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    832\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    835\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 230\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [130, 5]"
     ]
    }
   ],
   "source": [
    "#check accuracy metric\n",
    "tag_set = np.unique([t for tlist in train_labels for t in tlist])\n",
    "params = model_utils.Params('../data/base_params.json')\n",
    "diter=loader.data_iterator(train_data, params)\n",
    "with torch.no_grad():\n",
    "    for bdata, blabels in diter:\n",
    "        output = test_model(bdata)\n",
    "        print(np.unique(blabels))\n",
    "        acc = accuracy(output.numpy(), blabels.numpy())\n",
    "        f1 = f1_entity(output.numpy(), blabels.numpy())\n",
    "        print(f\"acc: {acc}\")\n",
    "        print(f\"f1: {f1}\")\n",
    "        pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-c5ae8de5c450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mblabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'blabels' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ORG': 0,\n",
       " 'O': 1,\n",
       " 'B-MISC': 2,\n",
       " 'B-PER': 3,\n",
       " 'I-PER': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-MISC': 7,\n",
       " 'I-LOC': 8,\n",
       " '<START>': 9,\n",
       " '<STOP>': 10}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"output size: {output.shape}\")\n",
    "print(f\"batch labels: {blabels.shape}\")\n",
    "blabels = blabels.numpy().ravel()\n",
    "output = np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "output size: torch.Size([130])\n",
      "batch labels: (130,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6211985688729875"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"-\"*80)\n",
    "print(f\"output size: {output.shape}\")\n",
    "print(f\"batch labels: {blabels.shape}\")\n",
    "f1_score(output, blabels, labels=tag_set, average='weighted')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 3, 4, 1, 5, 1, 1, 3, 4, 1, 5, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([130, 11])\n",
      "torch.Size([5, 26])\n"
     ]
    }
   ],
   "source": [
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    print(f\"label shape (bs, seq_len): {labels.shape}\")\n",
    "    labels = labels.ravel()\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0)\n",
    "\n",
    "    # np.argmax gives us the class predicted for each token by the model\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    print(f\"outputs size: {outputs.shape}\")\n",
    "    print(f\"raveled label size: {labels.shape}\")\n",
    "    pdb.set_trace()\n",
    "\n",
    "    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n",
    "    return np.sum(outputs==labels)/float(np.sum(mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predictions to a new column in `testb` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert the tagging scheme from BIO to IBO\n",
    "from nlp_utils import data_converter\n",
    "\n",
    "test_preds_ibo = data_converter.tags_to_conll(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the predicted labels for all words\n",
    "testb_data = data_converter.read_conll('../data/eng.testb')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check:  3683\n"
     ]
    }
   ],
   "source": [
    "augmented = data_converter.add_column(testb_data, test_preds_ibo)\n",
    "print(\"Sanity check: \", len(augmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data_converter.conll_to_data_stream(augmented, write_to_file='./testb_rnn1_preds.txt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
