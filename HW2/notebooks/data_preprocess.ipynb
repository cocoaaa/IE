{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "First we need to process given training and test datasets to a format that is more friendly to the machine learning framework. Let's start by understanding the raw inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/hayley/miniconda3/envs/fastai/lib/python36.zip', '/home/hayley/miniconda3/envs/fastai/lib/python3.6', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/lib-dynload', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/defusedxml-0.5.0-py3.6.egg', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/IPython/extensions', '/home/hayley/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import random\n",
    "print(sys.path)\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "# sklearn imports\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict\n",
    "\n",
    "# sklearn_crfsuite imports\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# train logging\n",
    "import logging\n",
    "from tqdm import trange\n",
    "# import .utils as my_utils\n",
    "# from nlp_utils.model_evaluate import evaluate\n",
    "# from nlp_utils import model_utils\n",
    "# set a random seed\n",
    "torch.manual_seed(10);\n",
    "\n",
    "# model saving and inspection\n",
    "import joblib\n",
    "import eli5\n",
    "\n",
    "import pdb \n",
    "\n",
    "# auto-reloads\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.20.0\n",
      "pytorch version: 0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "print(f\"pytorch version: {torch.__version__}\")\n",
    "# make sure we are using pytorch > 0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/hayley/miniconda3/envs/fastai/lib/python36.zip', '/home/hayley/miniconda3/envs/fastai/lib/python3.6', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/lib-dynload', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/defusedxml-0.5.0-py3.6.egg', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/IPython/extensions', '/home/hayley/.ipython', '..']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "hi\n",
      "hey\n"
     ]
    }
   ],
   "source": [
    "def nprint(*args):\n",
    "    print(\"=\"*80)\n",
    "    for arg in args:\n",
    "        print(arg)\n",
    "def test_nprint():\n",
    "    nprint(\"hi\", \"hey\")\n",
    "test_nprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets from the raw files\n",
    "We first collect sentences and labels from the raw files while tokenizing each sentence.\n",
    "For each sentence, we replace `<e1>` and `</e1>` with `E1_START` and `E1_END` tags, and `<e2>` and `</e2>` with `E2_START` and `E2_END`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define globals\n",
    "## Note space at the head or the tail\n",
    "E1_START = \"E1_START \"\n",
    "E1_END = \" E1_END \"\n",
    "E2_START = \"E2_START \"\n",
    "E2_END = \" E2_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hayley/Workspace/Class/IE/Relation-Classification/stanford/stanford-postagger-2017-06-09\n",
      "/home/hayley/Workspace/Class/IE/HW2/notebooks\n"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, abspath, join\n",
    "\n",
    "# Set up POS tagger and Tokenizer\n",
    "work_dir = !pwd\n",
    "work_dir = work_dir[0]\n",
    "postagger_path = '/home/hayley/Workspace/Class/IE/Relation-Classification/stanford/stanford-postagger-2017-06-09'\n",
    "print(postagger_path)\n",
    "print(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CLASSPATH'] = postagger_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively use Stanford coreNLP\n",
    "# from nltk.tokenize.stanford import StanfordTokenizer\n",
    "# from nltk.parse.corenlp import CoreNLPParser\n",
    "# parser = CoreNLPParser()\n",
    "\n",
    "# test\n",
    "# list(parser.tokenize('what ?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and test text files\n",
    "DATA_DIR = '../data'\n",
    "TRAIN_FPATH = abspath(join(DATA_DIR, 'SemEval2010_task8_training', 'TRAIN_FILE.TXT'))\n",
    "TEST_FPATH = abspath(join(DATA_DIR, 'SemEval2010_task8_testing', 'TEST_FILE.TXT'))\n",
    "\n",
    "PROC_DATA_DIR = abspath(join(DATA_DIR, 'Processed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPATH = TRAIN_FPATH #TEST_FPATH\n",
    "verbose = True\n",
    "def collect_sents_labels(fpath, verbose=False):\n",
    "    \"\"\"\n",
    "    Create a 2D sentences matrix and label vector from the input txt file \n",
    "    Args:\n",
    "    - fpath (str): path to the input data file (.txt)\n",
    "    - verbose (bool): True to show progress\n",
    "    \n",
    "    Returns:\n",
    "    - X (np.array or list): sentences collected from the input file\n",
    "        Each row is a sentence.  A sentence is a list of words.\n",
    "    - y (np.array or list): labels corresponding to the type of relation marked\n",
    "        at each sentence\n",
    "    \"\"\"\n",
    "        \n",
    "    sentences = []\n",
    "    y = []\n",
    "    for i,line in enumerate(open(fpath, 'r')):\n",
    "        line = line.rstrip()\n",
    "    #     if i < 10:\n",
    "    #         print(f\"{i}: {line}\")\n",
    "\n",
    "        if (i%4 == 0):\n",
    "            line = line.split(\"\\t\")[-1] # grab just the strings (ignore sentence index)\n",
    "            line = line[1:-1] # remove quotation marks\n",
    "\n",
    "            if i<2:\n",
    "                print(line)\n",
    "\n",
    "            # Replace XML tags\n",
    "            line = line.replace(\"<e1>\", E1_START).replace(\"</e1>\", E1_END)\n",
    "            line = line.replace(\"<e2>\", E2_START).replace(\"</e2>\", E2_END)\n",
    "\n",
    "            # Tokenize the string\n",
    "            line = StanfordTokenizer().tokenize(line)\n",
    "            sentences.append(line)\n",
    "\n",
    "            if i<2:\n",
    "                print(sentences[-1])\n",
    "\n",
    "        elif (i%4 == 1):\n",
    "            y.append(line)\n",
    "            \n",
    "        if verbose and i%100==0:\n",
    "            print(i)\n",
    "    assert (len(sentences) == len(y))\n",
    "    assert (len(y) == 8000)\n",
    "    \n",
    "    return (sentences, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents(fpath, verbose=False):\n",
    "    \"\"\"\n",
    "    Create a 2D array for sentences for input data without labels.\n",
    "    Args:\n",
    "    - fpath (str): path to the input data file (.txt)\n",
    "    - verbose (bool): True to show progress\n",
    "    \n",
    "    Returns:\n",
    "    - X (np.array or list): sentences collected from the input file\n",
    "        Each row is a sentence.  A sentence is a list of words.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for i,line in enumerate(open(fpath, 'r')):\n",
    "        # grab just the strings (ignore sentence index)\n",
    "        line = line.rstrip().split(\"\\t\")[-1]\n",
    "        # remove quotation marks\n",
    "        line = line[1:-1] \n",
    "\n",
    "        # Replace XML tags\n",
    "        line = line.replace(\"<e1>\", E1_START).replace(\"</e1>\", E1_END)\n",
    "        line = line.replace(\"<e2>\", E2_START).replace(\"</e2>\", E2_END)\n",
    "\n",
    "        # Tokenize the string\n",
    "        line = StanfordTokenizer().tokenize(line)\n",
    "        sentences.append(line)\n",
    "#         print(line)\n",
    "#         pdb.set_trace()\n",
    "\n",
    "        if i<2: print(sentences[-1]);\n",
    "            \n",
    "        if verbose and i%300==0: print(i);\n",
    "    \n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'most', 'common', 'E1_START', 'audits', 'E1_END', 'were', 'about', 'E2_START', 'waste', 'E2_END', 'and', 'recycling', '.']\n",
      "['The', 'E1_START', 'company', 'E1_END', 'fabricates', 'plastic', 'E2_START', 'chairs', 'E2_END', '.']\n"
     ]
    }
   ],
   "source": [
    "# train_dev_sents, train_dev_labels = collect_sents_labels(TRAIN_FPATH)\n",
    "TEST_FPATH = \"/home/hayley/Workspace/Class/IE/HW2/data/SemEval2010_task8_testing/TEST_FILE.txt\"\n",
    "test_sents = collect_sents(TEST_FPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joblib.dump(train_dev_sents, '../data/Processed/train_val_sents.pkl')\n",
    "joblib.dump(train_dev_labels, '../data/Processed/train_val_labels.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a relation to index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "['Cause-Effect(e1,e2)' 'Cause-Effect(e2,e1)' 'Component-Whole(e1,e2)'\n",
      " 'Component-Whole(e2,e1)' 'Content-Container(e1,e2)'\n",
      " 'Content-Container(e2,e1)' 'Entity-Destination(e1,e2)'\n",
      " 'Entity-Destination(e2,e1)' 'Entity-Origin(e1,e2)' 'Entity-Origin(e2,e1)'\n",
      " 'Instrument-Agency(e1,e2)' 'Instrument-Agency(e2,e1)'\n",
      " 'Member-Collection(e1,e2)' 'Member-Collection(e2,e1)'\n",
      " 'Message-Topic(e1,e2)' 'Message-Topic(e2,e1)' 'Product-Producer(e1,e2)'\n",
      " 'Product-Producer(e2,e1)' 'Other']\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "labelset = np.sort(np.unique(y))\n",
    "\n",
    "# Move 'Other' to the end of labelset\n",
    "o_idx = np.argwhere(labelset=='Other')\n",
    "labelset = np.append(np.delete(labelset, o_idx),['Other'])\n",
    "nprint(labelset, len(labelset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique relations:\n",
      " ['Cause-Effect(e1,e2)' 'Cause-Effect(e2,e1)' 'Component-Whole(e1,e2)'\n",
      " 'Component-Whole(e2,e1)' 'Content-Container(e1,e2)'\n",
      " 'Content-Container(e2,e1)' 'Entity-Destination(e1,e2)'\n",
      " 'Entity-Destination(e2,e1)' 'Entity-Origin(e1,e2)' 'Entity-Origin(e2,e1)'\n",
      " 'Instrument-Agency(e1,e2)' 'Instrument-Agency(e2,e1)'\n",
      " 'Member-Collection(e1,e2)' 'Member-Collection(e2,e1)'\n",
      " 'Message-Topic(e1,e2)' 'Message-Topic(e2,e1)' 'Product-Producer(e1,e2)'\n",
      " 'Product-Producer(e2,e1)' 'Other']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique relations:\\n {labelset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cause-Effect(e1,e2)': 0, 'Cause-Effect(e2,e1)': 1, 'Component-Whole(e1,e2)': 2, 'Component-Whole(e2,e1)': 3, 'Content-Container(e1,e2)': 4, 'Content-Container(e2,e1)': 5, 'Entity-Destination(e1,e2)': 6, 'Entity-Destination(e2,e1)': 7, 'Entity-Origin(e1,e2)': 8, 'Entity-Origin(e2,e1)': 9, 'Instrument-Agency(e1,e2)': 10, 'Instrument-Agency(e2,e1)': 11, 'Member-Collection(e1,e2)': 12, 'Member-Collection(e2,e1)': 13, 'Message-Topic(e1,e2)': 14, 'Message-Topic(e2,e1)': 15, 'Product-Producer(e1,e2)': 16, 'Product-Producer(e2,e1)': 17, 'Other': 18}\n"
     ]
    }
   ],
   "source": [
    "rel2idx = {rel:i for i,rel in enumerate(labelset)}\n",
    "print(rel2idx)\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "['Component-Whole(e2,e1)', 'Other', 'Instrument-Agency(e2,e1)', 'Other', 'Member-Collection(e1,e2)']\n",
      "================================================================================\n",
      "[3, 18, 11, 18, 12]\n"
     ]
    }
   ],
   "source": [
    "train_val_sents = sentences\n",
    "train_val_labels = y #string labels\n",
    "train_val_y = [rel2idx[rel] for rel in train_val_labels]\n",
    "nprint(train_val_labels[:5])\n",
    "nprint(train_val_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(train_val_labels) == len(train_val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.\n",
      "['The', 'system', 'as', 'described', 'above', 'has', 'its', 'greatest', 'application', 'in', 'an', 'arrayed', 'E1_START', 'configuration', 'E1_END', 'of', 'antenna', 'E2_START', 'elements', 'E2_END', '.']\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n"
     ]
    }
   ],
   "source": [
    "test_sents = collect_sents(TEST_FPATH, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'system',\n",
       "  'as',\n",
       "  'described',\n",
       "  'above',\n",
       "  'has',\n",
       "  'its',\n",
       "  'greatest',\n",
       "  'application',\n",
       "  'in',\n",
       "  'an',\n",
       "  'arrayed',\n",
       "  'E1_START',\n",
       "  'configuration',\n",
       "  'E1_END',\n",
       "  'of',\n",
       "  'antenna',\n",
       "  'E2_START',\n",
       "  'elements',\n",
       "  'E2_END',\n",
       "  '.'],\n",
       " [],\n",
       " ['omment'],\n",
       " ['nstrument-Agency', '-LRB-', 'e2', ',', 'e1'],\n",
       " ['A',\n",
       "  'misty',\n",
       "  'E1_START',\n",
       "  'ridge',\n",
       "  'E1_END',\n",
       "  'uprises',\n",
       "  'from',\n",
       "  'the',\n",
       "  'E2_START',\n",
       "  'surge',\n",
       "  'E2_END',\n",
       "  '.'],\n",
       " [],\n",
       " ['omment'],\n",
       " ['the'],\n",
       " ['The',\n",
       "  'current',\n",
       "  'view',\n",
       "  'is',\n",
       "  'that',\n",
       "  'the',\n",
       "  'chronic',\n",
       "  'E1_START',\n",
       "  'inflammation',\n",
       "  'E1_END',\n",
       "  'in',\n",
       "  'the',\n",
       "  'distal',\n",
       "  'part',\n",
       "  'of',\n",
       "  'the',\n",
       "  'stomach',\n",
       "  'caused',\n",
       "  'by',\n",
       "  'Helicobacter',\n",
       "  'pylori',\n",
       "  'E2_START',\n",
       "  'infection',\n",
       "  'E2_END',\n",
       "  'results',\n",
       "  'in',\n",
       "  'an',\n",
       "  'increased',\n",
       "  'acid',\n",
       "  'production',\n",
       "  'from',\n",
       "  'the',\n",
       "  'non-infected',\n",
       "  'upper',\n",
       "  'corpus',\n",
       "  'region',\n",
       "  'of',\n",
       "  'the',\n",
       "  'stomach',\n",
       "  '.'],\n",
       " []]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.'\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'system',\n",
       " 'as',\n",
       " 'described',\n",
       " 'above',\n",
       " 'has',\n",
       " 'its',\n",
       " 'greatest',\n",
       " 'application',\n",
       " 'in',\n",
       " 'an',\n",
       " 'arrayed',\n",
       " 'E1_START',\n",
       " 'configuration',\n",
       " 'E1_END',\n",
       " 'of',\n",
       " 'antenna',\n",
       " 'E2_START',\n",
       " 'elements',\n",
       " 'E2_END',\n",
       " '.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save results\n",
    "# joblib.dump(train_dev_sents, os.path.join(DATA_PROCDIR, 'train_dev_sents.pkl'))\n",
    "# joblib.dump(train_dev_labels, os.path.join(DATA_PROCDIR, 'train_dev_labels.pkl'))\n",
    "joblib.dump(train_dev_y, os.path.join(DATA_PROCDIR, 'train_dev_y.pkl'))\n",
    "\n",
    "joblib.dump(test_sents, os.path.join(DATA_PROCDIR, 'test_sents.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train_dev to train and dev dataset\n",
    "train_dev_indices = np.arange(len(train_dev_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features \n",
    "Now that we have the sentences and relations (labels) extracted from the input text files, the next step is to generate features with which we will create the data matrices for training and testing. I will generate three different kinds of features for the purpose of my experiments. \n",
    "\n",
    "1. Linguistic Features\n",
    "    - Part of Speech tags (POS)\n",
    "    - Word Embeddings\n",
    "    - WordNet tags\n",
    "    - Shortest Dependency Path (SDP)\n",
    "    - Grammar Relation tags (GR)\n",
    "\n",
    "2. Word Positional Indicitors\n",
    "Replace <e1></e1> with E1_START and E1_END.  Similarly, replace <e1></e1> with E1_START and E1_END\n",
    "\n",
    "3. Word Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Word Positional Indicators\n",
    "We replace <e1></e1> with E1_START and E1_END, and replace <e1></e1> with E1_START and E1_END.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for replacing <e1> -> e1_start, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Lastly, we use word positional embedding to encode the words between the two entities.  \n",
    "For example:\n",
    "\n",
    "<img/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for word positional embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
