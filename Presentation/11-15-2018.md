# Embedding entities and relations for learning and interence in Knowledge bases
Presenter: Hayley Song
# Topic:
- learning representations of entities and relations in KBs using the neural-embedding approach


# Three contributions
1. Presents a general framework for multi-relational learning that unifies most multi-relational embedding models (eg. NTN, TransE)
    - eneities = low-dim vectors learned from a NN
    - relations = bilinear and/or linear mapping 
2. Empirically evaluates different choices of entity representations and relation representations under this framework on link prediction task  
    - shows a simple bilinear formulation achieves new state-of-the-art
3. Introduce a **"embedding-based rule extraction"**: new approach to mine logical rules (eg. BornInCity(a,b) ^ CityInCountry(b,c) ->  Nationality(a,c)) using the learned relation embeddings
    - shows that such rules can be effectively extracted by modeling the composition of relation embeddings
    - embedding learned from the bilinear objective captures well compositional semantics or relaions via matrix multiplication
    - outperforms AMIE on mining rules that involve compositional reasoning

# Lesson learned
1. embedding learned from the bilinear objective are good at capturing relational semantics
2. composition of relations is characterized by matrix multiplication
3. their new embedding-based rule extraction approach achieves the state-of-the-art in mining Horn rules that involve compositional reasoning

# Set up
Large Knowledge bases: Freebase, DBPedia, YAGO
- entities and relations in RDF triple form (eg. (sub, pred, obj))
- really large: millions of entities, various relaions and billions of triples
- Why are these KBs interesting? 
  - they can be used to improve various tasks, eg. information retrieval, QA, biological data mining
  - "Relational learning": learning the relations between entities from these large knowledge bases. Use low-dim representations of entities and relations
    1. Tensor factorization
    2. Neural-embedding-based models [focus]
      - representations of entities and relations are learned using NN 
  Both have shown good scalability and reasoning (in terms of validating unseen facts given an existing KB) (aka. generalizability)

- Link prediction in KBs (aka. KG completion)
  1. KBs are incomplete: missing some entities and relations
  2. learn from local and global connectivity pattern from entities and relationships of different types at the same time
  3. Relation predicitions are then performed by using the learned patterns to generalize observed relationships between an entity of interest and all others

![IMAGE](resources/DF31657744B5DEF58484F74F00E43852.jpg =591x429)
![IMAGE](resources/019687C3CEEFA3ED1E9A3070DB84D847.jpg =560x423)
![IMAGE](resources/4D9A74C1B9F91D2751C776812BFAB3B9.jpg =554x424)

# Related work

## [Markov-logic networks]()
  - traditional statistical learning approach
  - does not scale well


## Different approach: embed multi-relational knowledge into low-dim representations of entities and relations 
- improved scalability 
- strong generalizability/reasoning on large-scale KBs
Consider a training set $S$ of triplets $(h,l,t)$ where $h,t \in E$ (set of entities) and $l \in L$ set of relations. 
1. [TransE](https://www.utc.fr/~bordesan/dokuwiki/_media/en/transe_nips13.pdf)
Relationships as translations in the embedding space (Bordes et al., 2013 NIPS)
  - **Key** 
  If $(h, l, t)$holds, then the embedding of the $t$ should be close to the $h$ plus some vector that depends on the relationship ($l$). Otherwise, $h+l$ should be far away from $t$
    ![IMAGE](resources/75B4F7CF2F45A697EAF55ED4290DE6B8.jpg =317x207)  
  
  - **Learning**
  Minimize a margin-based ranking criterion over the training set with norm-constraints on entity vectors: the L2-norm of the embeddings of the entities is 1
    ![IMAGE](resources/54F4E828D503F6718B4762F85A71047A.jpg =109x23)
    
    - This prevents the training process to trivially minimize $L$ by increating entity embedding norms proportionally
  
    - corrupted triplets: training triplets with either the head or tail replaced by a random entitiy (but not both at the same time)
  
    - margin-based loss:
      ![IMAGE](resources/D3B1FB95EC116B087978A0F9693ADA0F.jpg =441x84)
  
  - **Training demo**
  ![IMAGE](resources/78C5C43506E0286CCBD33001DAF444F4.jpg =537x293)![IMAGE](resources/7D0D90D30F9E04C379D2E46820617618.jpg =283x270)![IMAGE](resources/BC8538F9AAA2579605986EF8628B03AD.jpg =275x280)
  
  - **Prediction**
  Q: (Loyd Alexander, genre, ?), what is Loyd Alexander's genre?
  
    - Add the translation vector of "genre" relationship with "Loyd Alexander" entity vector and choose the nearest entity
  ![IMAGE](resources/6DDA0A064A8BF7A6372C414FBB46F834.jpg =475x289)

[Image credit](http://pyvandenbussche.info/2017/translating-embeddings-transe/)

2. [Neural Tensor Network (NTN)](https://nlp.stanford.edu/pubs/SocherChenManningNg_NIPS2013.pdf)
Goal: state whether two entities $(h,t)$ are in a certain relationship $l$ and with what certainty
    eg) $(h,l,t)$ = (Bental tiger, has part, tail) is true? with what certainty?
  **Overview**
  ![IMAGE](resources/736986B8A4D8CA062DA2466BDB17851E.jpg =450x244)
 
    **Key**
  Replace a standard linear NN layer with a bilinear tensor layer that relates the two entity vectors across multiple dimensions: "Neural tensor layer"
  eg:) tensor layer with two slices
  ![IMAGE](resources/4E5056F467D616DE3BE7862707362815.jpg =232x159)
  Math says:
  ![IMAGE](resources/4FAC55BDFA98255E09DDF3D4E94BD5F4.jpg =556x138)

    **vs. transE**
  ![IMAGE](resources/16E74F9799D2CE68D93FC4D27010374E.jpg =217x34)
  
  
    **Score function**
  ![IMAGE](resources/F87264D1723C353AFA6EDB7A39BCDAF5.jpg =416x57)
    
    **Training Objective**
  Same as in TransE (contrasitve max-margin)
  ![IMAGE](resources/96EB11CA75680F4A9C247CE0A8C7B058.jpg =379x54)
  
    **Two improvements**
      1. represent entities by the average  of its word vectors: *compositionality of language*
      eg:) vec(homo-sapiens) =  vec("hominid") + vec("sapiens")
      This can help with generalizability for predicting relations on unseen entities, eg. *homo-erectus*
      2. Initilize the word vectors with pre-trained vectors
      This takes advantage of general syntactic and semantic information from larger corpus.
      
  Images credit: [original paper](https://nlp.stanford.edu/pubs/SocherChenManningNg_NIPS2013.pdf)

---
# This paper
Notation: 
- Triplet: $(e_1, R, e_2$) where $e_1$ is the head entity (subject), R is the relation and $e_2$ is the tail (object)
- $y_{e} \in \mathcal{R}^n$: entity representation of entity $e$ in $n$-dimsion
- $Q_{r}$ (or $V_{r}$): $n \times m$ matrix (or $m$-dim vector)  for linear transformation $g_{r}^{a}$
- $T_{r} \in \mathcal{R}^{n \times n \times m}$: bilinear transformation for $g_{r}^{b}$

## Contribution1: General NN framework for multi-relational representation learning
1. Entity representations: project the input entity $(x_{e1},x_{e2})$ to $(y_{e1}, y_{e2})$:
  ![IMAGE](resources/A9C700E49197933A5CEFA6957ED20302.jpg =539x56)

2. Relation representations
Most existing scoring functions can be unified based on a basic linear transformation $g_{r}^{a}$, a bilinear transformation $g_{r}^{b}$, or their combination, where $g_{r}^{a}$ are $g_{r}^{b}$ are defined as :
![IMAGE](resources/A4436B7A649EA7C1F06B73A299F7F35B.jpg =448x58)

3. Summary of reformulated popular scoring functions
  ![IMAGE](resources/15984338D076006351DA6F183E4DEC6A.jpg =502x87)
  Note: 
    - NTN is the most expressive model: contains both linear and bilinear relation operators 
    - TransE is the simplest model (fewest params): only linear relation operators with one-dim vectors
    - Basic bilinear scoring function: ![IMAGE](resources/427C392656D432C446459FC346C7C9F3.jpg =155x29)
      - special case of NTN w/o non-libnear layer and the linear operators, uses 2-d matrix operator $M_r \in \mathcal{R}^{n \times n}$ rather than a tensor operator
      - Further constraint: $M_r$ is diagonal
  
4. Training objective
Minimize the margin-based ranking loss:
![IMAGE](resources/C8FEDD4804492A405C58E5E931D1F10D.jpg =369x61)

## Experiment 1
Link prediction: Predict the correctness of unseen triplets  
  - Dataset: WordNet (WN), Freebase (FB15k)
  - Metrics: Mean Reciprocal Rank(MRR), HITS@10 (top-10 accuracty), Mean Average Precision (MAP)
  - Models
    1. NTN with 4 tensor slices as in (Socher et al., 2013)
    2. Bilinear+Linear, NTN with 1 tensor slice and without the non-linear layer
    3. TransE, a special case of Bilinear+Linear
    4. Bilinear: using scoring function above
    5. Bilinear-diag: a special case of Bilinear where the relation matrix is a diagonal matrix
    
  - Result
  ![IMAGE](resources/FDA7A627EDC8BF44EB4C57A7A59A1CA5.jpg =449x139)
    - BILINEAR performs better than TransE, esp. on WN: captures more expressive relations
    - Multiplicative vs. Addivtive interactions
    Overall superior performance of BILINEAR-DIAG (*DISTMULT*)than TransE(*DISTADD*)
    ![IMAGE](resources/19B05FD5257C567260032615D0F3AB48.jpg =558x107)

## Contribution2: Embedding-based rule extraction
Use the learned embedding to extract logical rules from the KB
- Key is how to effectively explore the search space
- Proposed method's efficiency is affected by the number of distinct relation types (usually relatively small), not  by the size of the KB graph

- Limit: Restricted to Horn rules, eg:  
![IMAGE](resources/13933004E975CFE3BA02BE807959B4A0.jpg =400x35)
    

## Experiment 2. Rule extraction